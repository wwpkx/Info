---
mindmap-plugin: basic
---
# 矩阵分解
## 介绍
- 类似代数中的因式分解
- 将矩阵拆解为若干矩阵的乘积
- 矩阵分解结果可能对应缩放、旋转、投影、剪切等等各种几何变换
- 而**原矩阵的映射作用**就是这些**几何变换按特定次序的叠加**

## LU分解
- LU 分解 (lower–upper decomposition, LU decomposition)
- 将一个方阵 A，分解为一个下三角矩阵 (lower triangular matrix ) L 和一个上三角矩阵(upper triangular matrix) U 的乘积
- 可以视为高斯消元法 (Gaussian elimination) 的矩阵乘法形式
- A = LU
- scipy.linalg.lu()
    - A = PLU
    - P 为置换矩阵 (permutation matrix)
        - 置换矩阵的任意一行或列只有一个 1，剩余均为 0
        - 作用是交换矩阵的行、列
    - 矩阵 L 主对角线均为 1
    - 有很高的数值稳定性

## Cholesky分解
- Cholesky 分解 (Cholesky decomposition) 是 LU 分解的特例
- 适用于正定矩阵
- $A = LL^T$ 或 $A = R^T R$
- numpy.linalg.cholesky()
- $P=LL^T=R^TR=\begin{bmatrix}  1& cos\theta \\  cos\theta &1\end{bmatrix}=\begin{bmatrix}  1& 0 \\  cos\theta &sin\theta \end{bmatrix}\begin{bmatrix}  1& cos\theta \\  0 &sin\theta \end{bmatrix}$
- $R=\begin{bmatrix}  1& cos\theta \\  0 &sin\theta \end{bmatrix}=[r_1 \; r_2]$
	- $r_1=Re_1=\begin{bmatrix} 1\\0\end{bmatrix}$
	- $r_2=Re_2=\begin{bmatrix} cos\theta\\sin\theta \end{bmatrix}$
	- 开合，y轴旋转 到 与x轴$\theta$度夹角
- $\sum =\begin{bmatrix}  a^2 & abcos\theta \\  abcos\theta  & b^2\end{bmatrix}=\begin{bmatrix}  a &  \\    & b\end{bmatrix}\begin{bmatrix}  1 & cos\theta \\  cos\theta  & 1\end{bmatrix} \begin{bmatrix}  a &  \\    & b\end{bmatrix}=SR^TRS=(RS)^T(RS)$
	- RS, 先缩放S，再开合R

## LDL分解
- Cholesky 分解的扩展
- $A=LDL^T =LD^{\frac{1}{2}} (D^{\frac{1}{2}} )^TL^T =LD^{\frac{1}{2}} (LD^{\frac{1}{2}} )^T$
    - L 为下三角矩阵，但是对角线元素均为 1;
    - D 为对角矩阵，起到缩放作用
- scipy.linalg.ldl()    
    - 返回结果也包括置换矩阵

## QR分解
- QR 分解 (QR decomposition, QR factorization) 
- 两种常见形式
    - 完全型 (complete)
        - $X_{n\times D} =Q_{n\times n}R_{n\times D}$
        - Q 为方阵
        - 方阵 Q 为正交矩阵
        - x1 和 q1 平行，方向同向或反向, 和格拉姆-施密特正交化第一步一致
        - q1 是单位向量
    - 缩略型 (reduced)
        - $X_{n\times D} =Q_{n\times D}R_{D\times D}$
        - Q 和原矩阵形状相同
        - 列向量也两两正交
        - 正交矩阵的前提是矩阵为方阵

## 特征值分解
- 方阵A,  $Av = \lambda v$
- A=VΛV^{−1}
    - A 的特征分解 (eigen-decomposition)
    - Λ 被称作**特征值矩阵**
    - V 被称作**特征向量矩阵**
    - 例子
        - $Av_1 =\lambda_1v_1$
        - $Av_2 =\lambda_2v_2$
        - $A[v_1\; v_2]=[v_1 \; v_2]\begin{bmatrix}  \lambda_1&0 \\  0&\lambda_2\end{bmatrix}$
        - AV =VΛ
- 谱分解
    - 对称矩阵的特征值分解又叫谱分解 (spectral decomposition)
    - 格拉姆矩阵 $X^TX 和 XX^T$ 都是对称阵
    - $A=VΛV^T$
        - V 为正交矩阵
        
## SVD分解
- SVD 分解 (singular value decomposition)
- 适用于任何实数矩阵
- $X =USV^T$
    - S 主对角线元素$s_i$ 为奇异值 (singular value),  代表“缩放”矩阵
    - U 的列向量称作左奇异值向量 (left singular vector)
    - V 的列向量称作右奇异值向量 (right singular vector)
    - U 和 V 都是正交矩阵
- 四种类型
    - 完全型
        - U 和 V 为方阵，S 和 X 的形状相同    